{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- IMPORT MODULES, CLASSES AND METHODS --/#\n",
    "\n",
    "import zipfile                          #############################\n",
    "import os                               # || FILE SYSTEM / UTILS || #\n",
    "import copy                             #############################\n",
    "from prettytable import PrettyTable\n",
    "import copy\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np                  ###################################\n",
    "import pandas as pd                 # || EXPLORATIVE DATA ANALYSIS || #\n",
    "import matplotlib.pyplot as plt     ###################################\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "# https://towardsdatascience.com/handling-missing-data-like-a-pro-part-3-model-based-multiple-imputation-methods-bdfe85f93087 NumPyro, impyute,\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import sklearn\n",
    "import re\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn import naive_bayes                         #########################\n",
    "from sklearn import neural_network                      #  |-----------------|  #\n",
    "from sklearn import svm                                 # || MODEL SELECTION || #\n",
    "from sklearn import tree                                #  |-----------------|  #\n",
    "from sklearn import linear_model                        #########################\n",
    "\n",
    "# from PrunedCV import PrunedCV\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold     ##########################\n",
    "from sklearn.model_selection import ParameterGrid       # || MODEL VALIDATION || #\n",
    "                                                        ##########################\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- SET ENVIRONMENT --/#\n",
    "# Before starting we need to store the data properly. We define an ad-hoc folder where we will store everything.\n",
    "main_PATH = os.getcwd()\n",
    "\n",
    "# We check whether we already have the data.                        \n",
    "if 'data' not in os.listdir():                                      \n",
    "                                                                    \n",
    "    # Unzip files.\n",
    "    with zipfile.ZipFile(r'summer_project_dataset.zip') as zip_ref:\n",
    "\n",
    "        os.mkdir(main_PATH + '/data')   # We create the 'data' directory,\n",
    "        os.chdir(main_PATH + '/data')   # we change directory,\n",
    "    \n",
    "        data_PATH = os.getcwd()         # we get the data path\n",
    "        zip_ref.extractall(data_PATH)   # and we unzip there.       #####################\n",
    "                                                                    # || FILE SYSTEM || #    \n",
    "    file_PATH = data_PATH + '/summer_project_dataset'               #####################\n",
    "\n",
    "else:\n",
    "\n",
    "    # We just build the paths.\n",
    "    data_PATH = main_PATH + '/data'\n",
    "    file_PATH = data_PATH + '/summer_project_dataset'\n",
    "\n",
    "# Finally, we go back to our main path.\n",
    "os.chdir(main_PATH)\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# We also set a seed for reproducibility purposes.      #####################\n",
    "SEED = 42                                               # || RANDOM SEED || #\n",
    "np.random.seed(SEED)                                    #####################\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# LaTeX style plots.\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "# plt.rcParams['text.usetex']    = True         ############################\n",
    "# plt.rcParams['font.family']    = 'serif'      # || DEFAULT PARAMETERS || #\n",
    "# plt.rcParams['font.size']      = '10'         ############################\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "# pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2014/09/08/safest-cabbies-...</td>\n",
       "      <td>121</td>\n",
       "      <td>12</td>\n",
       "      <td>1015</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.545031</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160714</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2900</td>\n",
       "      <td>bus</td>\n",
       "      <td>tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/07/25/3d-printed-rifle/</td>\n",
       "      <td>532</td>\n",
       "      <td>9</td>\n",
       "      <td>503</td>\n",
       "      <td>0.569697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.737542</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157500</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1300</td>\n",
       "      <td>tech</td>\n",
       "      <td>thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/10/30/digital-dinosau...</td>\n",
       "      <td>435</td>\n",
       "      <td>9</td>\n",
       "      <td>232</td>\n",
       "      <td>0.646018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748428</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.427500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17700</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2014/08/27/homer-simpson-i...</td>\n",
       "      <td>134</td>\n",
       "      <td>12</td>\n",
       "      <td>171</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1500</td>\n",
       "      <td>bus</td>\n",
       "      <td>wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/10/creepy-robotic-...</td>\n",
       "      <td>728</td>\n",
       "      <td>11</td>\n",
       "      <td>286</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251786</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1400</td>\n",
       "      <td>tech</td>\n",
       "      <td>thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31710</th>\n",
       "      <td>http://mashable.com/2014/11/30/star-wars-guard...</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>440</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718978</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209167</td>\n",
       "      <td>-0.316667</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1000</td>\n",
       "      <td>world</td>\n",
       "      <td>tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31711</th>\n",
       "      <td>http://mashable.com/2014/11/14/uk-floods/</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.700</td>\n",
       "      <td>11000</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31712</th>\n",
       "      <td>http://mashable.com/2014/09/08/paypal-bitcoin-...</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "      <td>969</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710623</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2400</td>\n",
       "      <td>tech</td>\n",
       "      <td>tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31713</th>\n",
       "      <td>http://mashable.com/2013/08/23/mashable-androi...</td>\n",
       "      <td>503</td>\n",
       "      <td>11</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.412308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.621080</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323413</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>6000</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31714</th>\n",
       "      <td>http://mashable.com/2014/09/24/designers-dilem...</td>\n",
       "      <td>106</td>\n",
       "      <td>13</td>\n",
       "      <td>261</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206019</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1500</td>\n",
       "      <td>socmed</td>\n",
       "      <td>wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31715 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  timedelta  \\\n",
       "0      http://mashable.com/2014/09/08/safest-cabbies-...        121   \n",
       "1       http://mashable.com/2013/07/25/3d-printed-rifle/        532   \n",
       "2      http://mashable.com/2013/10/30/digital-dinosau...        435   \n",
       "3      http://mashable.com/2014/08/27/homer-simpson-i...        134   \n",
       "4      http://mashable.com/2013/01/10/creepy-robotic-...        728   \n",
       "...                                                  ...        ...   \n",
       "31710  http://mashable.com/2014/11/30/star-wars-guard...         37   \n",
       "31711          http://mashable.com/2014/11/14/uk-floods/         52   \n",
       "31712  http://mashable.com/2014/09/08/paypal-bitcoin-...        121   \n",
       "31713  http://mashable.com/2013/08/23/mashable-androi...        503   \n",
       "31714  http://mashable.com/2014/09/24/designers-dilem...        106   \n",
       "\n",
       "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0                  12              1015         0.422018               1.0   \n",
       "1                   9               503         0.569697               1.0   \n",
       "2                   9               232         0.646018               1.0   \n",
       "3                  12               171         0.722892               1.0   \n",
       "4                  11               286         0.652632               1.0   \n",
       "...               ...               ...              ...               ...   \n",
       "31710              11               440         0.564103               1.0   \n",
       "31711              14                 0         0.000000               0.0   \n",
       "31712               9               969         0.489583               1.0   \n",
       "31713              11              1976         0.412308               1.0   \n",
       "31714              13               261         0.616279               1.0   \n",
       "\n",
       "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  ...  \\\n",
       "0                      0.545031         10               6      33.0  ...   \n",
       "1                      0.737542          9               0       NaN  ...   \n",
       "2                      0.748428         12               3       4.0  ...   \n",
       "3                      0.867925          9               5       0.0  ...   \n",
       "4                      0.800000          5               2       NaN  ...   \n",
       "...                         ...        ...             ...       ...  ...   \n",
       "31710                  0.718978         10               2       NaN  ...   \n",
       "31711                  0.000000          0               0       0.0  ...   \n",
       "31712                  0.710623          6               5       2.0  ...   \n",
       "31713                  0.621080         21               3       1.0  ...   \n",
       "31714                  0.780822          5               4       1.0  ...   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0                  -0.160714              -0.500000              -0.071429   \n",
       "1                  -0.157500              -0.250000              -0.100000   \n",
       "2                  -0.427500              -1.000000              -0.187500   \n",
       "3                  -0.216667              -0.250000              -0.166667   \n",
       "4                  -0.251786              -0.500000              -0.100000   \n",
       "...                      ...                    ...                    ...   \n",
       "31710              -0.209167              -0.316667              -0.050000   \n",
       "31711               0.000000               0.000000               0.000000   \n",
       "31712              -0.400000              -1.000000              -0.050000   \n",
       "31713              -0.323413              -1.000000              -0.050000   \n",
       "31714              -0.206019              -0.312500              -0.150000   \n",
       "\n",
       "       title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                0.000000                     0.000                0.500000   \n",
       "1                0.000000                     0.000                0.500000   \n",
       "2                0.000000                     0.000                0.500000   \n",
       "3                0.400000                    -0.250                0.100000   \n",
       "4                0.200000                    -0.100                0.300000   \n",
       "...                   ...                       ...                     ...   \n",
       "31710            0.000000                     0.000                0.500000   \n",
       "31711            0.666667                    -0.700                0.166667   \n",
       "31712            0.000000                     0.000                0.500000   \n",
       "31713            0.700000                    -0.400                0.200000   \n",
       "31714            0.325000                     0.175                0.175000   \n",
       "\n",
       "       abs_title_sentiment_polarity  shares  data_channel    weekday  \n",
       "0                             0.000    2900           bus    tuesday  \n",
       "1                             0.000    1300          tech   thursday  \n",
       "2                             0.000   17700     lifestyle  wednesday  \n",
       "3                             0.250    1500           bus  wednesday  \n",
       "4                             0.100    1400          tech   thursday  \n",
       "...                             ...     ...           ...        ...  \n",
       "31710                         0.000    1000         world    tuesday  \n",
       "31711                         0.700   11000     lifestyle     monday  \n",
       "31712                         0.000    2400          tech    tuesday  \n",
       "31713                         0.400    6000     lifestyle     friday  \n",
       "31714                         0.175    1500        socmed  wednesday  \n",
       "\n",
       "[31715 rows x 49 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\\-- DATASET LOADING AND PREPROCESSING --/#\n",
    "# Aome variables are stored as float, but they are actually int. Two reasons why:\n",
    "#       -) nan values are considered as float --> first estimate them and then change the data type.\n",
    "#       -) there are inconsistencies, especially in kw_max_min where some int values are float instead.\n",
    "# for the moment just let's store everything as float, but further inspections are needed.\n",
    "\n",
    "data_types = {\n",
    "              'url' : str, 'timedelta' : int, 'shares' : int, 'data_channel' : str, 'weekday' : str, \n",
    "              \n",
    "              'n_tokens_title'          : int, 'n_tokens_content'       : int, 'n_unique_tokens' : float, 'n_non_stop_words' : float,\n",
    "              'n_non_stop_unique_tokens': float, 'average_token_length' : float,\n",
    "\n",
    "              'num_hrefs' : int, 'num_self_hrefs' : int, 'num_imgs' : float, 'num_videos' : float,\n",
    "              \n",
    "              'kw_min_min' : float, 'kw_max_min' : float, 'kw_avg_min' : float, 'kw_min_max' : float, 'kw_max_max'   : float,\n",
    "              'kw_avg_max' : float, 'kw_min_avg' : float, 'kw_max_avg' : float, 'kw_avg_avg' : float, 'num_keywords' : float,\n",
    "              \n",
    "              'self_reference_min_shares' : float, 'self_reference_max_shares' : float, 'self_reference_avg_sharess' : float,\n",
    "              \n",
    "              'LDA_00' : float, 'LDA_01' : float, 'LDA_02' : float, 'LDA_03' : float, 'LDA_04' : float,\n",
    "              \n",
    "              'global_subjectivity' : float, 'global_sentiment_polarity' : float, 'global_rate_positive_words' : float, 'global_rate_negative_words' : float,\n",
    "              \n",
    "              'rate_positive_words' : float, 'rate_negative_words' : float,\n",
    "              \n",
    "              'avg_positive_polarity' : float, 'min_positive_polarity' : float, 'max_positive_polarity' : float, 'avg_negative_polarity' : float,\n",
    "              'min_negative_polarity' : float, 'max_negative_polarity' : float,\n",
    "\n",
    "              'title_subjectivity' : float, 'title_sentiment_polarity' : float, 'abs_title_subjectivity' : float, 'abs_title_sentiment_polarity' : float,\n",
    "              }                                                    \n",
    "                                                                   \n",
    "                                                                   \n",
    "df = pd.read_csv(file_PATH + r'/development.csv',                 \n",
    "                   usecols = lambda column: column != 'id', dtype = data_types)              \n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Before isolation forest\n",
    "* Discard n_tokens_content = 0.\n",
    "* Discard -1 values.\n",
    "* Discard all 0 values for kw_avg_min, kw_avg_avg, kw_avg_max --> keyword_behaviour.\n",
    "* Drop kw_max_max, kw_max_min, kw_min_min, kw_max_max, kw_max_avg, kw_min_avg\n",
    "* Drop n_non_stop_words, n_non_stop_unique_tokens (, n_tokens_content?).\n",
    "* Drop self_reference_min_shares, self_reference_max_shares.\n",
    "* Drop rate_positive_words, rate_negative_words.\n",
    "* Drop abs_title_subjectivity, abs_title_sentiment_polarity.\n",
    "* Drop URL.\n",
    "* Fill imgs, videos, keywords.\n",
    "* Encode weekdays into Weekend and Not Weekend.\n",
    "* Make linear combination between avg, min, max positive/negative polarity (weighted average: 0.6, 0.2, 0.2 / 0.7, 0.15, 0.15).\n",
    "* Make a combination between title_subjectivity, title_sentiment_polarity (title_subjectivity * title_sentiment_polarity)\n",
    "* Make a combination between global_subjectivity, global_sentiment_polarity (global_subjectivity * global_sentiment_polarity)\n",
    "* Find a way to combine the previous statistic (keyword_behaiour) with num_keywords.\n",
    "* Try with/without timedelta.\n",
    "\n",
    "# Apply isolation forest\n",
    "* Give scores to samples and discard outliers over a certain threshold.\n",
    "\n",
    "# After isolation forest\n",
    "* Apply logarithm IF NECESSARY (analyze residuals and check whether a logarithm may be useful).\n",
    "\n",
    "# Models\n",
    "* Negative Binomial\n",
    "* Log-normal\n",
    "* Log and then simple regression\n",
    "* Gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import Preprocessing\n",
    "\n",
    "pre = Preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_zeros = ['n_tokens_content']\n",
    "\n",
    "for feature in discard_zeros:\n",
    "\n",
    "    _ = pre.discard_zeros(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_negative = ['kw_avg_min', 'kw_avg_avg', 'kw_avg_max']\n",
    "\n",
    "for feature in discard_negative:\n",
    "\n",
    "    _ = pre.discard_negatives(feature, include_zeros = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[('n_non_stop_words', 'n_non_stop_unique_tokens', 'self_reference_min_shares', 'self_reference_max_shares', 'rate_positive_words', 'rate_negative_words', 'abs_title_subjectivity', 'abs_title_sentiment_polarity', 'url')] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m drop_columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mn_non_stop_words\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mn_non_stop_unique_tokens\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mself_reference_min_shares\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mself_reference_max_shares\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrate_positive_words\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mrate_negative_words\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mabs_title_subjectivity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mabs_title_sentiment_polarity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m pre\u001b[39m.\u001b[39;49mdrop(drop_columns)\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/preprocessor.py:18\u001b[0m, in \u001b[0;36mPreprocessing.drop\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\u001b[39mself\u001b[39m, columns: \u001b[39mlist\u001b[39m \u001b[39m=\u001b[39m []) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m     15\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m    Takes as input the list of columns to drop and returns the dataframe.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__dataframe__ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__dataframe__\u001b[39m.\u001b[39;49mdrop([columns], axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__dataframe__\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/pandas/core/frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[1;32m   5111\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   5112\u001b[0m     labels: IndexLabel \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5120\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5122\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5256\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5257\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   5259\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   5260\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5261\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   5262\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   5263\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   5264\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   5265\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   5266\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/pandas/core/generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4548\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4549\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4551\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4552\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/pandas/core/generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m   4590\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4591\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4592\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4594\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4595\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6696\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6694\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m   6695\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 6696\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6697\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m   6698\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[('n_non_stop_words', 'n_non_stop_unique_tokens', 'self_reference_min_shares', 'self_reference_max_shares', 'rate_positive_words', 'rate_negative_words', 'abs_title_subjectivity', 'abs_title_sentiment_polarity', 'url')] not found in axis\""
     ]
    }
   ],
   "source": [
    "drop_columns = ['n_non_stop_words', 'n_non_stop_unique_tokens', 'self_reference_min_shares', 'self_reference_max_shares', 'rate_positive_words',\n",
    "                'rate_negative_words', 'abs_title_subjectivity', 'abs_title_sentiment_polarity', 'url']\n",
    "\n",
    "pre.drop(drop_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
