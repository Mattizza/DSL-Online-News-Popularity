{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- IMPORT MODULES, CLASSES AND METHODS --/#\n",
    "\n",
    "import zipfile                          #############################\n",
    "import os                               # || FILE SYSTEM / UTILS || #\n",
    "import copy                             #############################\n",
    "from prettytable import PrettyTable\n",
    "import copy\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import numpy as np                  ###################################\n",
    "import pandas as pd                 # || EXPLORATIVE DATA ANALYSIS || #\n",
    "import matplotlib.pyplot as plt     ###################################\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "# https://towardsdatascience.com/handling-missing-data-like-a-pro-part-3-model-based-multiple-imputation-methods-bdfe85f93087 NumPyro, impyute,\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import sklearn\n",
    "import re\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn import naive_bayes                         #########################\n",
    "from sklearn import neural_network                      #  |-----------------|  #\n",
    "from sklearn import svm                                 # || MODEL SELECTION || #\n",
    "from sklearn import tree                                #  |-----------------|  #\n",
    "from sklearn import linear_model                        #########################\n",
    "\n",
    "# from PrunedCV import PrunedCV\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold     ##########################\n",
    "from sklearn.model_selection import ParameterGrid       # || MODEL VALIDATION || #\n",
    "                                                        ##########################\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- SET ENVIRONMENT --/#\n",
    "# Before starting we need to store the data properly. We define an ad-hoc folder where we will store everything.\n",
    "main_PATH = os.getcwd()\n",
    "\n",
    "# We check whether we already have the data.                        \n",
    "if 'data' not in os.listdir():                                      \n",
    "                                                                    \n",
    "    # Unzip files.\n",
    "    with zipfile.ZipFile(r'summer_project_dataset.zip') as zip_ref:\n",
    "\n",
    "        os.mkdir(main_PATH + '/data')   # We create the 'data' directory,\n",
    "        os.chdir(main_PATH + '/data')   # we change directory,\n",
    "    \n",
    "        data_PATH = os.getcwd()         # we get the data path\n",
    "        zip_ref.extractall(data_PATH)   # and we unzip there.       #####################\n",
    "                                                                    # || FILE SYSTEM || #    \n",
    "    file_PATH = data_PATH + '/summer_project_dataset'               #####################\n",
    "\n",
    "else:\n",
    "\n",
    "    # We just build the paths.\n",
    "    data_PATH = main_PATH + '/data'\n",
    "    file_PATH = data_PATH + '/summer_project_dataset'\n",
    "\n",
    "# Finally, we go back to our main path.\n",
    "os.chdir(main_PATH)\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# We also set a seed for reproducibility purposes.      #####################\n",
    "SEED = 42                                               # || RANDOM SEED || #\n",
    "np.random.seed(SEED)                                    #####################\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# LaTeX style plots.\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "# plt.rcParams['text.usetex']    = True         ############################\n",
    "# plt.rcParams['font.family']    = 'serif'      # || DEFAULT PARAMETERS || #\n",
    "# plt.rcParams['font.size']      = '10'         ############################\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "# pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- DATASET LOADING AND PREPROCESSING --/#\n",
    "# Aome variables are stored as float, but they are actually int. Two reasons why:\n",
    "#       -) nan values are considered as float --> first estimate them and then change the data type.\n",
    "#       -) there are inconsistencies, especially in kw_max_min where some int values are float instead.\n",
    "# for the moment just let's store everything as float, but further inspections are needed.\n",
    "\n",
    "data_types = {\n",
    "              'url' : str, 'timedelta' : int, 'shares' : int, 'data_channel' : str, 'weekday' : str, \n",
    "              \n",
    "              'n_tokens_title'          : int, 'n_tokens_content'       : int, 'n_unique_tokens' : float, 'n_non_stop_words' : float,\n",
    "              'n_non_stop_unique_tokens': float, 'average_token_length' : float,\n",
    "\n",
    "              'num_hrefs' : int, 'num_self_hrefs' : int, 'num_imgs' : float, 'num_videos' : float,\n",
    "              \n",
    "              'kw_min_min' : float, 'kw_max_min' : float, 'kw_avg_min' : float, 'kw_min_max' : float, 'kw_max_max'   : float,\n",
    "              'kw_avg_max' : float, 'kw_min_avg' : float, 'kw_max_avg' : float, 'kw_avg_avg' : float, 'num_keywords' : float,\n",
    "              \n",
    "              'self_reference_min_shares' : float, 'self_reference_max_shares' : float, 'self_reference_avg_sharess' : float,\n",
    "              \n",
    "              'LDA_00' : float, 'LDA_01' : float, 'LDA_02' : float, 'LDA_03' : float, 'LDA_04' : float,\n",
    "              \n",
    "              'global_subjectivity' : float, 'global_sentiment_polarity' : float, 'global_rate_positive_words' : float, 'global_rate_negative_words' : float,\n",
    "              \n",
    "              'rate_positive_words' : float, 'rate_negative_words' : float,\n",
    "              \n",
    "              'avg_positive_polarity' : float, 'min_positive_polarity' : float, 'max_positive_polarity' : float, 'avg_negative_polarity' : float,\n",
    "              'min_negative_polarity' : float, 'max_negative_polarity' : float,\n",
    "\n",
    "              'title_subjectivity' : float, 'title_sentiment_polarity' : float, 'abs_title_subjectivity' : float, 'abs_title_sentiment_polarity' : float,\n",
    "              }                                                    \n",
    "                                                                   \n",
    "                                                                   \n",
    "df = pd.read_csv(file_PATH + r'/development.csv',                 \n",
    "                   usecols = lambda column: column != 'id', dtype = data_types)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop all the correlated or useless features and we perform the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(['shares'], axis=1)\n",
    "y = df['shares']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 3000, random_state = 42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the test aside and we never touch it. We then perform a train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = 2000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data: 31715\n",
      "Train set size: 26715\t\t(84.23%)\n",
      "Validation set size: 2000\t(6.31%)\n",
      "Test set size: 3000\t\t(9.46%)\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial data: {df.shape[0]}\\nTrain set size: {X_train.shape[0]}\\t\\t({(X_train.shape[0] / df.shape[0]) * 100:.2f}%)\\n\\\n",
    "Validation set size: {X_validation.shape[0]}\\t({(X_validation.shape[0] / df.shape[0]) * 100:.2f}%)\\n\\\n",
    "Test set size: {X_test.shape[0]}\\t\\t({(X_test.shape[0] / df.shape[0]) * 100:.2f}%)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose these proportions for three important reasons:\n",
    "* More training data for more complex models\n",
    "* The validation data are not that important, we need them just to tune the hyperparameters.\n",
    "* The test data are more important in order to evaluate the model, so we need more of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {value: index for index, value in enumerate(df.columns)}\n",
    "mapping\n",
    "\n",
    "class columnDropperTransformer():\n",
    "    def __init__(self,columns):\n",
    "        self.columns=columns\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        return X.drop(self.columns,axis=1)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DROP FEATURES\n",
    "drop_features = ['kw_max_min', 'kw_max_avg', 'kw_min_min', 'url', 'timedelta', 'n_non_stop_words',\n",
    "                'n_tokens_content', 'n_non_stop_unique_tokens', 'self_reference_max_shares',\n",
    "                'self_reference_min_shares', 'rate_positive_words', 'rate_negative_words',\n",
    "                'max_positive_polarity', 'min_positive_polarity', 'min_negative_polarity',\n",
    "                'max_negative_polarity', 'abs_title_subjectivity', 'abs_title_sentiment_polarity',\n",
    "                'kw_min_max', 'kw_max_max', 'kw_min_avg']\n",
    "drop_indices = [mapping[value] for value in drop_features]\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# APPLY LOGARITHMS\n",
    "log1p_features = ['num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'kw_avg_min', 'kw_avg_avg', 'self_reference_avg_sharess']\n",
    "log1p_indices = [mapping[value] for value in log1p_features]\n",
    "\n",
    "log_feature = ['shares']\n",
    "log_index = [mapping[value] for value in log_feature]\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# DISCRETIZE KW_AVG_MAX\n",
    "conditions = [\n",
    "              (np.log1p(df['kw_avg_max']) < 2),\n",
    "              ((np.log1p(df['kw_avg_max']) < 11) & (np.log1p(df['kw_avg_max']) > 0)),\n",
    "              (np.log1p(df['kw_avg_max']) > 11)\n",
    "]\n",
    "\n",
    "labels = ['kw_avg_max_none', 'kw_avg_max_medium', 'kw_avg_max_high']\n",
    "\n",
    "df['kw_avg_max'] = np.where(conditions[0], labels[0],\n",
    "                            np.where(conditions[1], labels[1],\n",
    "                                     np.where(conditions[2], labels[2], None)))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# DISCRETIZE TITLE_SUBJECTIVITY AND TITLE_SENTIMENT_POLARITY\n",
    "subjectivity_bin_values = {\n",
    "                           1 : 'no_subjectivity',\n",
    "                           2 : 'low_subjectivity',\n",
    "                           3 : 'medium_subjectvity',\n",
    "                           4 : 'high_subjectivity'\n",
    "                          }\n",
    "\n",
    "subjectivity_bins = [-0.000001, 0.000001, 0.334, 0.667, 1]\n",
    "\n",
    "\n",
    "df['title_sentiment_polarity'] = pd.cut(df['title_sentiment_polarity'],\n",
    "                                        bins=[-1.00001, -0.5, -0.000000001, +0.000000001, 0.5, 1],\n",
    "                                        labels=['high_negative_polarity', 'low_negative_polarity', 'neutral_polarity', 'low_positive_polarity', 'high_positive_polarity'],\n",
    "                                        right = True)\n",
    "\n",
    "polarity_bin_values = {\n",
    "                           1 : 'high_negative_polarity',\n",
    "                           2 : 'low_negative_polarity',\n",
    "                           3 : 'neutral_polarity',\n",
    "                           4 : 'low_positive_polarity',\n",
    "                           5 : 'high_positive_polarity'\n",
    "                          }\n",
    "\n",
    "polarity_bins = [-1.00001, -0.5, -0.000000001, +0.000000001, 0.5, 1]\n",
    "\n",
    "# Define a function to assign values to bins\n",
    "def discretize_title_subjectivity(X):\n",
    "    bin_indices = np.digitize(X[:, 0], subjectivity_bins, right = True)\n",
    "    bin_labels = np.array([subjectivity_bin_values[i] for i in bin_indices]).reshape(-1, 1)\n",
    "    return bin_labels\n",
    "\n",
    "def discretize_title_sentiment_polarity(X):\n",
    "    bin_indices = np.digitize(X[:, 0], polarity_bins, right = True)\n",
    "    bin_labels = np.array([polarity_bin_values[i] for i in bin_indices]).reshape(-1, 1)\n",
    "    return bin_labels\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# DISCRETIZE WEEKDAY\n",
    "df['weekday'] = np.where(df['weekday'].isin(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']), 'Not Weekend', 'Weekend')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# APPLY PIPLINE\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessing', ColumnTransformer([\n",
    "        (\"columnDropper\", columnDropperTransformer, drop_features),                                                                             # Drop useless/correlated features.\n",
    "        ('log1p', FunctionTransformer(func = np.log1p, inverse_func = np.expm1, validate = False), log1p_indices),  # Apply log1p.\n",
    "        ('log', FunctionTransformer(func = np.log, inverse_func = np.exp, validate = False), log_index),            # Apply logp.\n",
    "        ('discretize_subjectivity', FunctionTransformer(func = discretize_title_subjectivity, validate = False), [42]),\n",
    "        ('discretize_sentiment_polarity', FunctionTransformer(func = discretize_title_sentiment_polarity, validate = False), [43]),\n",
    "        ('onehot', OneHotEncoder(categories='auto', sparse=False, handle_unknown='ignore'), [18, 42, 43, 47, 48]),\n",
    "        ('regressor', GradientBoostingRegressor())],\n",
    "    remainder = 'passthrough'))\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preprocessed_data \u001b[39m=\u001b[39m preprocessing_pipeline\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:694\u001b[0m, in \u001b[0;36mColumnTransformer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit all transformers using X.\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \n\u001b[1;32m    678\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39m    This estimator.\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[39m# we use fit_transform to make sure to set sparse_output_ (for which we\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[39m# need the transformed data) to have consistent output type in predict\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(X, y\u001b[39m=\u001b[39;49my)\n\u001b[1;32m    695\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:723\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[39m# set n_features_in_ attribute\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 723\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_transformers()\n\u001b[1;32m    724\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    725\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:398\u001b[0m, in \u001b[0;36mColumnTransformer._validate_transformers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers:\n\u001b[1;32m    396\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m names, transformers, _ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers)\n\u001b[1;32m    400\u001b[0m \u001b[39m# validate names\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_names(names)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# preprocessed_data = preprocessing_pipeline.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
