{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- IMPORT MODULES, CLASSES AND METHODS --/#\n",
    "\n",
    "import zipfile                          #############################\n",
    "import os                               # || FILE SYSTEM / UTILS || #\n",
    "import copy                             #############################\n",
    "from prettytable import PrettyTable\n",
    "import copy\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import numpy as np                  ###################################\n",
    "import pandas as pd                 # || EXPLORATIVE DATA ANALYSIS || #\n",
    "import matplotlib.pyplot as plt     ###################################\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "# https://towardsdatascience.com/handling-missing-data-like-a-pro-part-3-model-based-multiple-imputation-methods-bdfe85f93087 NumPyro, impyute,\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import sklearn\n",
    "import re\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "from Pruned import PrunedCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn import naive_bayes                         #########################\n",
    "from sklearn import neural_network                      #  |-----------------|  #\n",
    "from sklearn import svm                                 # || MODEL SELECTION || #\n",
    "from sklearn import tree                                #  |-----------------|  #\n",
    "from sklearn import linear_model                        #########################\n",
    "\n",
    "# from PrunedCV import PrunedCV\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold     ##########################\n",
    "from sklearn.model_selection import ParameterGrid       # || MODEL VALIDATION || #\n",
    "                                                        ##########################\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from preprocessing import Preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- SET ENVIRONMENT --/#\n",
    "# Before starting we need to store the data properly. We define an ad-hoc folder where we will store everything.\n",
    "main_PATH = os.getcwd()\n",
    "\n",
    "# We check whether we already have the data.                        \n",
    "if 'data' not in os.listdir():                                      \n",
    "                                                                    \n",
    "    # Unzip files.\n",
    "    with zipfile.ZipFile(r'summer_project_dataset.zip') as zip_ref:\n",
    "\n",
    "        os.mkdir(main_PATH + '/data')   # We create the 'data' directory,\n",
    "        os.chdir(main_PATH + '/data')   # we change directory,\n",
    "    \n",
    "        data_PATH = os.getcwd()         # we get the data path\n",
    "        zip_ref.extractall(data_PATH)   # and we unzip there.       #####################\n",
    "                                                                    # || FILE SYSTEM || #    \n",
    "    file_PATH = data_PATH + '/summer_project_dataset'               #####################\n",
    "\n",
    "else:\n",
    "\n",
    "    # We just build the paths.\n",
    "    data_PATH = main_PATH + '/data'\n",
    "    file_PATH = data_PATH + '/summer_project_dataset'\n",
    "\n",
    "# Finally, we go back to our main path.\n",
    "os.chdir(main_PATH)\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# We also set a seed for reproducibility purposes.      #####################\n",
    "SEED = 42                                               # || RANDOM SEED || #\n",
    "np.random.seed(SEED)                                    #####################\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# LaTeX style plots.\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "# plt.rcParams['text.usetex']    = True         ############################\n",
    "# plt.rcParams['font.family']    = 'serif'      # || DEFAULT PARAMETERS || #\n",
    "# plt.rcParams['font.size']      = '10'         ############################\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "# pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- DATASET LOADING AND PREPROCESSING --/#\n",
    "# Aome variables are stored as float, but they are actually int. Two reasons why:\n",
    "#       -) nan values are considered as float --> first estimate them and then change the data type.\n",
    "#       -) there are inconsistencies, especially in kw_max_min where some int values are float instead.\n",
    "# for the moment just let's store everything as float, but further inspections are needed.\n",
    "\n",
    "data_types = {\n",
    "              'url' : str, 'timedelta' : int, 'shares' : int, 'data_channel' : str, 'weekday' : str, \n",
    "              \n",
    "              'n_tokens_title'          : int, 'n_tokens_content'       : int, 'n_unique_tokens' : float, 'n_non_stop_words' : float,\n",
    "              'n_non_stop_unique_tokens': float, 'average_token_length' : float,\n",
    "\n",
    "              'num_hrefs' : int, 'num_self_hrefs' : int, 'num_imgs' : float, 'num_videos' : float,\n",
    "              \n",
    "              'kw_min_min' : float, 'kw_max_min' : float, 'kw_avg_min' : float, 'kw_min_max' : float, 'kw_max_max'   : float,\n",
    "              'kw_avg_max' : float, 'kw_min_avg' : float, 'kw_max_avg' : float, 'kw_avg_avg' : float, 'num_keywords' : float,\n",
    "              \n",
    "              'self_reference_min_shares' : float, 'self_reference_max_shares' : float, 'self_reference_avg_sharess' : float,\n",
    "              \n",
    "              'LDA_00' : float, 'LDA_01' : float, 'LDA_02' : float, 'LDA_03' : float, 'LDA_04' : float,\n",
    "              \n",
    "              'global_subjectivity' : float, 'global_sentiment_polarity' : float, 'global_rate_positive_words' : float, 'global_rate_negative_words' : float,\n",
    "              \n",
    "              'rate_positive_words' : float, 'rate_negative_words' : float,\n",
    "              \n",
    "              'avg_positive_polarity' : float, 'min_positive_polarity' : float, 'max_positive_polarity' : float, 'avg_negative_polarity' : float,\n",
    "              'min_negative_polarity' : float, 'max_negative_polarity' : float,\n",
    "\n",
    "              'title_subjectivity' : float, 'title_sentiment_polarity' : float, 'abs_title_subjectivity' : float, 'abs_title_sentiment_polarity' : float,\n",
    "              }                                                    \n",
    "                                                                   \n",
    "                                                                   \n",
    "df = pd.read_csv(file_PATH + r'/development.csv',                 \n",
    "                   usecols = lambda column: column != 'id', dtype = data_types)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validation/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(['shares'], axis=1)\n",
    "y = df['shares']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 3000, random_state = 42)\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = 2000, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Initial data: {df.shape[0]}\\nTrain set size: {X_train.shape[0]}\\t\\t({(X_train.shape[0] / df.shape[0]) * 100:.2f}%)\\n\\\n",
    "# Validation set size: {X_validation.shape[0]}\\t({(X_validation.shape[0] / df.shape[0]) * 100:.2f}%)\\n\\\n",
    "# Test set size: {X_test.shape[0]}\\t\\t({(X_test.shape[0] / df.shape[0]) * 100:.2f}%)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose these proportions for three important reasons:\n",
    "* More training data for more complex models\n",
    "* The validation data are not that important, we need them just to tune the hyperparameters.\n",
    "* The test data are more important in order to evaluate the model, so we need more of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y, means: dict = {}, train = True):\n",
    "    \n",
    "    if train:\n",
    "        X['shares'] = y\n",
    "        X = copy.deepcopy(X[(X['shares'] > 300) & (X['shares'] < 22000)])\n",
    "        y = copy.deepcopy(y[(y > 300) & (y < 22000)])\n",
    "        X = X.drop('shares', axis = 1)\n",
    "\n",
    "    conditions = [\n",
    "              (np.log1p(X['kw_avg_max']) < 2),\n",
    "              ((np.log1p(X['kw_avg_max']) < 11) & (np.log1p(X['kw_avg_max']) > 0)),\n",
    "              (np.log1p(X['kw_avg_max']) > 11)\n",
    "              ]\n",
    "\n",
    "    labels = ['kw_avg_max_none', 'kw_avg_max_medium', 'kw_avg_max_high']\n",
    "\n",
    "    columns_to_fill = ['num_imgs', 'num_videos', 'num_keywords']\n",
    "    dict_means = {}\n",
    "\n",
    "    if train:\n",
    "        X[columns_to_fill] = X[columns_to_fill].fillna(X[columns_to_fill].mean())\n",
    "        dict_means = {'num_imgs'     : X['num_imgs'].mean(),\n",
    "                      'num_videos'   : X['num_videos'].mean(),\n",
    "                      'num_keywords' : X['num_keywords'].mean()}\n",
    "    \n",
    "    else:\n",
    "        X = X.fillna(means)\n",
    "\n",
    "    X['kw_avg_max'] = np.where(conditions[0], labels[0],\n",
    "                               np.where(conditions[1], labels[1],\n",
    "                                        np.where(conditions[2], labels[2], None)))\n",
    "    \n",
    "    X['weekday'] = np.where(X['weekday'].isin(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']), 'Not Weekend', 'Weekend')\n",
    "\n",
    "    if train:\n",
    "        y = y[X['n_tokens_content'] != 0]\n",
    "        X = X[X['n_tokens_content'] != 0]\n",
    "\n",
    "        y = y[np.log1p(X['num_hrefs']) < 4]\n",
    "        y = y[np.log1p(X['num_self_hrefs']) < 3]\n",
    "        y = y[X['self_reference_avg_sharess'] < 50000]\n",
    "        \n",
    "        X = X[np.log1p(X['num_hrefs']) < 4]\n",
    "        X = X[np.log1p(X['num_self_hrefs']) < 3]\n",
    "        X = X[X['self_reference_avg_sharess'] < 50000]\n",
    "        \n",
    "    X['title_sentiment_polarity'] = pd.cut(X['title_sentiment_polarity'],\n",
    "                                           bins=[-1.00001, -0.5, -0.000000001, +0.000000001, 0.5, 1],\n",
    "                                           labels=['high_negative_polarity', 'low_negative_polarity', 'neutral_polarity', 'low_positive_polarity', 'high_positive_polarity'],\n",
    "                                           right = True)\n",
    "    \n",
    "    X['title_subjectivity'] = pd.cut(X['title_subjectivity'],\n",
    "                                     bins = [-0.000001, 0.000001, 0.334, 0.667, 1],\n",
    "                                     labels = ['no_subjectivity', 'low_subjectivity', 'medium_subjectvity', 'high_subjectivity'],\n",
    "                                     right = True)\n",
    "\n",
    "    X_processed = X.drop(['kw_max_min', 'kw_max_avg', 'kw_min_min', 'url', 'timedelta', 'n_non_stop_words',\n",
    "                          'n_tokens_content', 'n_non_stop_unique_tokens', 'self_reference_max_shares',\n",
    "                          'self_reference_min_shares', 'rate_positive_words', 'rate_negative_words',\n",
    "                          'max_positive_polarity', 'min_positive_polarity', 'min_negative_polarity',\n",
    "                          'max_negative_polarity', 'abs_title_subjectivity', 'abs_title_sentiment_polarity',\n",
    "                          'kw_min_max', 'kw_max_max', 'kw_min_avg'], axis = 1)\n",
    "    \n",
    "    X_processed['num_hrefs'] = np.log1p(X_processed['num_hrefs'])\n",
    "    X_processed['num_self_hrefs'] = np.log1p(X_processed['num_self_hrefs'])\n",
    "    X_processed['num_imgs'] = np.log1p(X_processed['num_imgs'])\n",
    "    X_processed['num_videos'] = np.log1p(X_processed['num_videos'])\n",
    "    X_processed['kw_avg_min'] = np.log1p(X_processed['kw_avg_min'] + 1)\n",
    "    X_processed['kw_avg_avg'] = np.log1p(X_processed['kw_avg_avg'])\n",
    "    X_processed['self_reference_avg_sharess'] = np.log1p(X_processed['self_reference_avg_sharess'])\n",
    "    \n",
    "    y_processed = np.log(y)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['data_channel'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('data_channel', axis = 1)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['weekday'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('weekday', axis = 1)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['kw_avg_max'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('kw_avg_max', axis = 1)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['title_subjectivity'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('title_subjectivity', axis = 1)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['title_sentiment_polarity'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('title_sentiment_polarity', axis = 1)\n",
    "\n",
    "    return X_processed, y_processed, dict_means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr_X_train, pr_y_train, means = preprocess(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr_X_validation, pr_y_validation, _ = preprocess(X_validation, y_validation, means, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # DROP FEATURES\n",
    "# drop_features = ['kw_max_min', 'kw_max_avg', 'kw_min_min', 'url', 'timedelta', 'n_non_stop_words',\n",
    "#                 'n_tokens_content', 'n_non_stop_unique_tokens', 'self_reference_max_shares',\n",
    "#                 'self_reference_min_shares', 'rate_positive_words', 'rate_negative_words',\n",
    "#                 'max_positive_polarity', 'min_positive_polarity', 'min_negative_polarity',\n",
    "#                 'max_negative_polarity', 'abs_title_subjectivity', 'abs_title_sentiment_polarity',\n",
    "#                 'kw_min_max', 'kw_max_max', 'kw_min_avg']\n",
    "# drop_indices = [mapping[value] for value in drop_features]\n",
    "\n",
    "# # -------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # APPLY LOGARITHMS\n",
    "# log1p_features = ['num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'kw_avg_min', 'kw_avg_avg', 'self_reference_avg_sharess']\n",
    "# log1p_indices = [mapping[value] for value in log1p_features]\n",
    "\n",
    "# log_feature = ['shares']\n",
    "# log_index = [mapping[value] for value in log_feature]\n",
    "\n",
    "# # -------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # DISCRETIZE KW_AVG_MAX\n",
    "# conditions = [\n",
    "#               (np.log1p(df['kw_avg_max']) < 2),\n",
    "#               ((np.log1p(df['kw_avg_max']) < 11) & (np.log1p(df['kw_avg_max']) > 0)),\n",
    "#               (np.log1p(df['kw_avg_max']) > 11)\n",
    "# ]\n",
    "\n",
    "# labels = ['kw_avg_max_none', 'kw_avg_max_medium', 'kw_avg_max_high']\n",
    "\n",
    "# df['kw_avg_max'] = np.where(conditions[0], labels[0],\n",
    "#                             np.where(conditions[1], labels[1],\n",
    "#                                      np.where(conditions[2], labels[2], None)))\n",
    "\n",
    "# # ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # DISCRETIZE TITLE_SUBJECTIVITY AND TITLE_SENTIMENT_POLARITY\n",
    "# subjectivity_bin_values = {\n",
    "#                            1 : 'no_subjectivity',\n",
    "#                            2 : 'low_subjectivity',\n",
    "#                            3 : 'medium_subjectvity',\n",
    "#                            4 : 'high_subjectivity'\n",
    "#                           }\n",
    "\n",
    "# subjectivity_bins = [-0.000001, 0.000001, 0.334, 0.667, 1]\n",
    "\n",
    "\n",
    "# df['title_sentiment_polarity'] = pd.cut(df['title_sentiment_polarity'],\n",
    "#                                         bins=[-1.00001, -0.5, -0.000000001, +0.000000001, 0.5, 1],\n",
    "#                                         labels=['high_negative_polarity', 'low_negative_polarity', 'neutral_polarity', 'low_positive_polarity', 'high_positive_polarity'],\n",
    "#                                         right = True)\n",
    "\n",
    "# polarity_bin_values = {\n",
    "#                            1 : 'high_negative_polarity',\n",
    "#                            2 : 'low_negative_polarity',\n",
    "#                            3 : 'neutral_polarity',\n",
    "#                            4 : 'low_positive_polarity',\n",
    "#                            5 : 'high_positive_polarity'\n",
    "#                           }\n",
    "\n",
    "# polarity_bins = [-1.00001, -0.5, -0.000000001, +0.000000001, 0.5, 1]\n",
    "\n",
    "# # Define a function to assign values to bins\n",
    "# def discretize_title_subjectivity(X):\n",
    "#     bin_indices = np.digitize(X[:, 0], subjectivity_bins, right = True)\n",
    "#     bin_labels = np.array([subjectivity_bin_values[i] for i in bin_indices]).reshape(-1, 1)\n",
    "#     return bin_labels\n",
    "\n",
    "# def discretize_title_sentiment_polarity(X):\n",
    "#     bin_indices = np.digitize(X[:, 0], polarity_bins, right = True)\n",
    "#     bin_labels = np.array([polarity_bin_values[i] for i in bin_indices]).reshape(-1, 1)\n",
    "#     return bin_labels\n",
    "\n",
    "# # ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # DISCRETIZE WEEKDAY\n",
    "# df['weekday'] = np.where(df['weekday'].isin(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']), 'Not Weekend', 'Weekend')\n",
    "\n",
    "# # ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # APPLY PIPLINE\n",
    "# preprocessing_pipeline = Pipeline([\n",
    "#     ('preprocessing', ColumnTransformer([                                                                             # Drop useless/correlated features.\n",
    "#         ('log1p', FunctionTransformer(func = np.log1p, inverse_func = np.expm1, validate = False), log1p_indices),  # Apply log1p.\n",
    "#         ('log', FunctionTransformer(func = np.log, inverse_func = np.exp, validate = False), log_index),            # Apply logp.\n",
    "#         ('discretize_subjectivity', FunctionTransformer(func = discretize_title_subjectivity, validate = False), [42]),\n",
    "#         ('discretize_sentiment_polarity', FunctionTransformer(func = discretize_title_sentiment_polarity, validate = False), [43]),\n",
    "#         ('onehot', OneHotEncoder(categories='auto', sparse=False, handle_unknown='ignore'), [18, 42, 43, 46, 47]),\n",
    "#         (\"columnDropper\", columnDropperTransformer, drop_features)],\n",
    "#     remainder = 'passthrough'))\n",
    "#     ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pr_X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m params \u001b[39m=\u001b[39m {                            \u001b[39m# Here we specify only the SVM, but there should be a lot of other configurations.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.05\u001b[39m, \u001b[39m0.2\u001b[39m],\n\u001b[1;32m      5\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mn_estimator\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m500\u001b[39m, \u001b[39m1000\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mmax_features\u001b[39m\u001b[39m'\u001b[39m : [\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msqrt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlog2\u001b[39m\u001b[39m'\u001b[39m]           \n\u001b[1;32m     12\u001b[0m         }\n\u001b[1;32m     14\u001b[0m grad_bost_regr \u001b[39m=\u001b[39m RandomForestRegressor()\n\u001b[0;32m---> 15\u001b[0m grad_bost_regr\u001b[39m.\u001b[39mfit(pr_X_train, pr_y_train)\n\u001b[1;32m     16\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(grad_bost_regr\u001b[39m.\u001b[39mpredict(pr_X_validation))\n\u001b[1;32m     17\u001b[0m rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(mean_squared_error(np\u001b[39m.\u001b[39mexp(pr_y_validation), y_pred))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pr_X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {                            # Here we specify only the SVM, but there should be a lot of other configurations.\n",
    "          'learning_rate': [0.05, 0.2],\n",
    "          'n_estimator': [500, 1000],\n",
    "          'min_samples_leaf' : [5, 15, 20],\n",
    "          'min_weight_fraction_leaf' : [0.0, 0.1, 0.2],\n",
    "          'max_depth' : [3, 4, 5],\n",
    "          'subsample' : [0.5, 1.0],\n",
    "          'min_samples_split' : [5, 10, 20],\n",
    "          'max_features' : ['auto', 'sqrt', 'log2']           \n",
    "        }\n",
    "\n",
    "grad_bost_regr = RandomForestRegressor()\n",
    "grad_bost_regr.fit(pr_X_train, pr_y_train)\n",
    "y_pred = np.exp(grad_bost_regr.predict(pr_X_validation))\n",
    "rmse = np.sqrt(mean_squared_error(np.exp(pr_y_validation), y_pred))\n",
    "print(f'GradientBoostingRegressor without PCA')\n",
    "print(f'rmse : {rmse}')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pr_X_train_pca = pca.fit_transform(pr_X_train)\n",
    "# Transform the test data using the trained PCA model\n",
    "pr_X_validation_pca = pca.transform(pr_X_validation)\n",
    "regr_pca = RandomForestRegressor()\n",
    "regr_pca.fit(pr_X_train_pca, pr_y_train)\n",
    "y_pred_pca = np.exp(regr_pca.predict(pr_X_validation_pca))\n",
    "mse_pca = mean_squared_error(np.exp(pr_y_validation), y_pred_pca)\n",
    "rmse_pca = np.sqrt(mse_pca)\n",
    "print(f'GradientBoostingRegressor with PCA')\n",
    "print(f'rmse_pca : {rmse_pca}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from Pruned import PrunedCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#\\-- A BIT OF TUNING AND THEN SELECTION --/#\n",
    "\n",
    "# We first define the grid of parameters. We need to specify both the values AND the name of the classifier because it will be of use\n",
    "# when picking the right ones.\n",
    "# param_grid = {\n",
    "\n",
    "#     'sklearn.ensemble.GradientBoostingRegressor': {                            # Here we specify only the SVM, but there should be a lot of other configurations.\n",
    "#                 'learning_rate': [0.05, 0.1, 0.15],\n",
    "#                 'n_estimator': [100, 300, 500, 1000, 2000],\n",
    "#                 'min_samples_leaf' : [5, 15, 20],\n",
    "#                 'min_weight_fraction_leaf' : [0.0, 0.1, 0.2],\n",
    "#                 'max_depth' : [2, 3, 4, 5],\n",
    "#                 'subsample' : [0.5, 0.75, 1.0],\n",
    "#                 'min_samples_split' : [2, 5, 10, 20],\n",
    "#                 'max_features' : ['auto', 'sqrt', 'log2']           \n",
    "#                }                                     \n",
    "             \n",
    "#              }\n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    'sklearn.ensemble.HistGradientBoostingRegressor': {                            # Here we specify only the SVM, but there should be a lot of other configurations.\n",
    "                         \n",
    "               },\n",
    "    'sklearn.ensemble.RandomForestRegressor' : {},\n",
    "    'sklearn.svm.SVR' : {}\n",
    "                                      \n",
    "             \n",
    "             }\n",
    "\n",
    "scores      = [mean_squared_error] # List of scores we want to compute, e.g. accuracy_score, f1_score, ... They must be methods.\n",
    "THRESH_SKIP = 2\n",
    "THRESH_PERCENTAGE = 0.95\n",
    "MAIN_SCORE = 'mean_squared_error'\n",
    "\n",
    "# Create a StratifiedKFold object for Cross Validation and perform the train/test split.\n",
    "skf = KFold(n_splits = 10, shuffle = True, random_state = SEED)\n",
    "\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Start the Cross Validation. In this first step we simply try some configurations and try to tune the models the best we can.\n",
    "# Notice that this phase should take not much time, but given the amount of possible configurations it is likely to be quite expensive.\n",
    "# However, we are sure that at the end we will get a fair comparison between every model. \n",
    "# This will be called Cross-VSV (Validation-Selection-Validation) or Cross-SeleDation (Selection-Validation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sklearn.ensemble.HistGradientBoostingRegressor\n",
      "\n",
      "\n",
      "\tNEW CONFIGURATION\n",
      "\n",
      "Configuration: {}\n",
      "\n",
      "Fold 1 / 10 - Skip: 0 / 5\n",
      "Results: {'mean_squared_error': 7638.93535442127}\n",
      "Highest average mean_squared_error: 20000\n",
      "Fold 2 / 10 - Skip: 0 / 5\n",
      "Results: {'mean_squared_error': 14479.831591780865}\n",
      "Highest average mean_squared_error: 20000\n",
      "Fold 3 / 10 - Skip: 0 / 5\n",
      "Results: {'mean_squared_error': 8567.779564441209}\n",
      "Highest average mean_squared_error: 20000\n"
     ]
    }
   ],
   "source": [
    "# p = PCA()\n",
    "# temp = copy.deepcopy(X_train)\n",
    "# p.fit(temp)\n",
    "\n",
    "cross_validator = PrunedCV(X_train, y_train, skf)\n",
    "\n",
    "\n",
    "cross_validator.set_params(param_grid, scores)\n",
    "cross_validator.set_evaluation(mean_squared_error, 5, 1.20)\n",
    "toc = time.time()\n",
    "cross_validator.do_cross_validation(verbose = 4)\n",
    "\n",
    "\n",
    "# At the end we will have a fair comparison between models. We can store additional information regarding the Standard Deviation as well.\n",
    "\n",
    "# Consider using PrettyTable() to visualize the results.\n",
    "# t = PrettyTable()\n",
    "# t.field_names = ['Model', 'Accuracy', 'F1']\n",
    "# t.add_row([name, accuracy, f1])\n",
    "\n",
    "# print(t)\n",
    "\n",
    "tic = time.time()\n",
    "print(tic - toc)\n",
    "# CONSIDER TO PRUNE THE SEARCH IF THE RESULTS DON'T IMPROVE MUCH.\n",
    "# Example: if the best result with one model is 0.95 and another configuration of the same model is stuck near 0.90, just don't\n",
    "# wait for the end of the loop but go to the next model/configuration.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
