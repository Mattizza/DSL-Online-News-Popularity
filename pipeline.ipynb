{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- IMPORT MODULES, CLASSES AND METHODS --/#\n",
    "\n",
    "import zipfile                          #############################\n",
    "import os                               # || FILE SYSTEM / UTILS || #\n",
    "import copy                             #############################\n",
    "from prettytable import PrettyTable\n",
    "import copy\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import numpy as np                  ###################################\n",
    "import pandas as pd                 # || EXPLORATIVE DATA ANALYSIS || #\n",
    "import matplotlib.pyplot as plt     ###################################\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "# https://towardsdatascience.com/handling-missing-data-like-a-pro-part-3-model-based-multiple-imputation-methods-bdfe85f93087 NumPyro, impyute,\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import sklearn\n",
    "import re\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "from Pruned import PrunedCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn import naive_bayes                         #########################\n",
    "from sklearn import neural_network                      #  |-----------------|  #\n",
    "from sklearn import svm                                 # || MODEL SELECTION || #\n",
    "from sklearn import tree                                #  |-----------------|  #\n",
    "from sklearn import linear_model                        #########################\n",
    "\n",
    "# from PrunedCV import PrunedCV\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold     ##########################\n",
    "from sklearn.model_selection import ParameterGrid       # || MODEL VALIDATION || #\n",
    "                                                        ##########################\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from preprocessing import Preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- SET ENVIRONMENT --/#\n",
    "# Before starting we need to store the data properly. We define an ad-hoc folder where we will store everything.\n",
    "main_PATH = os.getcwd()\n",
    "\n",
    "# We check whether we already have the data.                        \n",
    "if 'data' not in os.listdir():                                      \n",
    "                                                                    \n",
    "    # Unzip files.\n",
    "    with zipfile.ZipFile(r'summer_project_dataset.zip') as zip_ref:\n",
    "\n",
    "        os.mkdir(main_PATH + '/data')   # We create the 'data' directory,\n",
    "        os.chdir(main_PATH + '/data')   # we change directory,\n",
    "    \n",
    "        data_PATH = os.getcwd()         # we get the data path\n",
    "        zip_ref.extractall(data_PATH)   # and we unzip there.       #####################\n",
    "                                                                    # || FILE SYSTEM || #    \n",
    "    file_PATH = data_PATH + '/summer_project_dataset'               #####################\n",
    "\n",
    "else:\n",
    "\n",
    "    # We just build the paths.\n",
    "    data_PATH = main_PATH + '/data'\n",
    "    file_PATH = data_PATH + '/summer_project_dataset'\n",
    "\n",
    "# Finally, we go back to our main path.\n",
    "os.chdir(main_PATH)\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# We also set a seed for reproducibility purposes.      #####################\n",
    "SEED = 42                                               # || RANDOM SEED || #\n",
    "np.random.seed(SEED)                                    #####################\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# LaTeX style plots.\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "# plt.rcParams['text.usetex']    = True         ############################\n",
    "# plt.rcParams['font.family']    = 'serif'      # || DEFAULT PARAMETERS || #\n",
    "# plt.rcParams['font.size']      = '10'         ############################\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "# pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\-- DATASET LOADING AND PREPROCESSING --/#\n",
    "# Aome variables are stored as float, but they are actually int. Two reasons why:\n",
    "#       -) nan values are considered as float --> first estimate them and then change the data type.\n",
    "#       -) there are inconsistencies, especially in kw_max_min where some int values are float instead.\n",
    "# for the moment just let's store everything as float, but further inspections are needed.\n",
    "\n",
    "data_types = {\n",
    "              'url' : str, 'timedelta' : int, 'shares' : int, 'data_channel' : str, 'weekday' : str, \n",
    "              \n",
    "              'n_tokens_title'          : int, 'n_tokens_content'       : int, 'n_unique_tokens' : float, 'n_non_stop_words' : float,\n",
    "              'n_non_stop_unique_tokens': float, 'average_token_length' : float,\n",
    "\n",
    "              'num_hrefs' : int, 'num_self_hrefs' : int, 'num_imgs' : float, 'num_videos' : float,\n",
    "              \n",
    "              'kw_min_min' : float, 'kw_max_min' : float, 'kw_avg_min' : float, 'kw_min_max' : float, 'kw_max_max'   : float,\n",
    "              'kw_avg_max' : float, 'kw_min_avg' : float, 'kw_max_avg' : float, 'kw_avg_avg' : float, 'num_keywords' : float,\n",
    "              \n",
    "              'self_reference_min_shares' : float, 'self_reference_max_shares' : float, 'self_reference_avg_sharess' : float,\n",
    "              \n",
    "              'LDA_00' : float, 'LDA_01' : float, 'LDA_02' : float, 'LDA_03' : float, 'LDA_04' : float,\n",
    "              \n",
    "              'global_subjectivity' : float, 'global_sentiment_polarity' : float, 'global_rate_positive_words' : float, 'global_rate_negative_words' : float,\n",
    "              \n",
    "              'rate_positive_words' : float, 'rate_negative_words' : float,\n",
    "              \n",
    "              'avg_positive_polarity' : float, 'min_positive_polarity' : float, 'max_positive_polarity' : float, 'avg_negative_polarity' : float,\n",
    "              'min_negative_polarity' : float, 'max_negative_polarity' : float,\n",
    "\n",
    "              'title_subjectivity' : float, 'title_sentiment_polarity' : float, 'abs_title_subjectivity' : float, 'abs_title_sentiment_polarity' : float,\n",
    "              }                                                    \n",
    "                                                                   \n",
    "                                                                   \n",
    "df = pd.read_csv(file_PATH + r'/development.csv',                 \n",
    "                   usecols = lambda column: column != 'id', dtype = data_types)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validation/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(['shares'], axis=1)\n",
    "y = df['shares']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 3000, random_state = 42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = 2000, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data: 31715\n",
      "Train set size: 26715\t\t(84.23%)\n",
      "Validation set size: 2000\t(6.31%)\n",
      "Test set size: 3000\t\t(9.46%)\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial data: {df.shape[0]}\\nTrain set size: {X_train.shape[0]}\\t\\t({(X_train.shape[0] / df.shape[0]) * 100:.2f}%)\\n\\\n",
    "Validation set size: {X_validation.shape[0]}\\t({(X_validation.shape[0] / df.shape[0]) * 100:.2f}%)\\n\\\n",
    "Test set size: {X_test.shape[0]}\\t\\t({(X_test.shape[0] / df.shape[0]) * 100:.2f}%)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose these proportions for three important reasons:\n",
    "* More training data for more complex models\n",
    "* The validation data are not that important, we need them just to tune the hyperparameters.\n",
    "* The test data are more important in order to evaluate the model, so we need more of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y, means: dict = {}, train = True):\n",
    "    \n",
    "    if train:\n",
    "        X['shares'] = y\n",
    "        X = copy.deepcopy(X[(X['shares'] > 300) & (X['shares'] < 22000)])\n",
    "        print(y.shape)\n",
    "        y = copy.deepcopy(y[(y > 300) & (y < 22000)])\n",
    "        print(y.shape)\n",
    "        X = X.drop('shares', axis = 1)\n",
    "\n",
    "    conditions = [\n",
    "              (np.log1p(X['kw_avg_max']) < 2),\n",
    "              ((np.log1p(X['kw_avg_max']) < 11) & (np.log1p(X['kw_avg_max']) > 0)),\n",
    "              (np.log1p(X['kw_avg_max']) > 11)\n",
    "              ]\n",
    "\n",
    "    labels = ['kw_avg_max_none', 'kw_avg_max_medium', 'kw_avg_max_high']\n",
    "\n",
    "    columns_to_fill = ['num_imgs', 'num_videos', 'num_keywords']\n",
    "    dict_means = {}\n",
    "\n",
    "    if train:\n",
    "        X[columns_to_fill] = X[columns_to_fill].fillna(X[columns_to_fill].mean())\n",
    "        dict_means = {'num_imgs'     : X['num_imgs'].mean(),\n",
    "                      'num_videos'   : X['num_videos'].mean(),\n",
    "                      'num_keywords' : X['num_keywords'].mean()}\n",
    "    \n",
    "    else:\n",
    "        X = X.fillna(means)\n",
    "\n",
    "    X['kw_avg_max'] = np.where(conditions[0], labels[0],\n",
    "                               np.where(conditions[1], labels[1],\n",
    "                                        np.where(conditions[2], labels[2], None)))\n",
    "    \n",
    "    X['weekday'] = np.where(X['weekday'].isin(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']), 'Not Weekend', 'Weekend')\n",
    "\n",
    "    y = y[X['n_tokens_content'] != 0]\n",
    "    X = X[X['n_tokens_content'] != 0]\n",
    "    \n",
    "    y = y[np.log1p(X['num_hrefs']) < 4]\n",
    "    y = y[np.log1p(X['num_self_hrefs']) < 3]\n",
    "    y = y[X['self_reference_avg_sharess'] < 50000]\n",
    "    \n",
    "    X = X[np.log1p(X['num_hrefs']) < 4]\n",
    "    X = X[np.log1p(X['num_self_hrefs']) < 3]\n",
    "    X = X[X['self_reference_avg_sharess'] < 50000]\n",
    "\n",
    "    X['title_sentiment_polarity'] = pd.cut(X['title_sentiment_polarity'],\n",
    "                                           bins=[-1.00001, -0.5, -0.000000001, +0.000000001, 0.5, 1],\n",
    "                                           labels=['high_negative_polarity', 'low_negative_polarity', 'neutral_polarity', 'low_positive_polarity', 'high_positive_polarity'],\n",
    "                                           right = True)\n",
    "    \n",
    "    X['title_subjectivity'] = pd.cut(X['title_subjectivity'],\n",
    "                                     bins = [-0.000001, 0.000001, 0.334, 0.667, 1],\n",
    "                                     labels = ['no_subjectivity', 'low_subjectivity', 'medium_subjectvity', 'high_subjectivity'],\n",
    "                                     right = True)\n",
    "\n",
    "    X_processed = X.drop(['kw_max_min', 'kw_max_avg', 'kw_min_min', 'url', 'timedelta', 'n_non_stop_words',\n",
    "                          'n_tokens_content', 'n_non_stop_unique_tokens', 'self_reference_max_shares',\n",
    "                          'self_reference_min_shares', 'rate_positive_words', 'rate_negative_words',\n",
    "                          'max_positive_polarity', 'min_positive_polarity', 'min_negative_polarity',\n",
    "                          'max_negative_polarity', 'abs_title_subjectivity', 'abs_title_sentiment_polarity',\n",
    "                          'kw_min_max', 'kw_max_max', 'kw_min_avg'], axis = 1)\n",
    "    \n",
    "    X_processed['num_hrefs'] = np.log1p(X_processed['num_hrefs'])\n",
    "    X_processed['num_self_hrefs'] = np.log1p(X_processed['num_self_hrefs'])\n",
    "    X_processed['num_imgs'] = np.log1p(X_processed['num_imgs'])\n",
    "    X_processed['num_videos'] = np.log1p(X_processed['num_videos'])\n",
    "    X_processed['kw_avg_min'] = np.log1p(X_processed['kw_avg_min'] + 1)\n",
    "    X_processed['kw_avg_avg'] = np.log1p(X_processed['kw_avg_avg'])\n",
    "    X_processed['self_reference_avg_sharess'] = np.log1p(X_processed['self_reference_avg_sharess'])\n",
    "    \n",
    "    y_processed = np.log(y)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['data_channel'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('data_channel', axis = 1)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['weekday'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('weekday', axis = 1)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['kw_avg_max'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('kw_avg_max', axis = 1)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['title_subjectivity'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('title_subjectivity', axis = 1)\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(X_processed['title_sentiment_polarity'])\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    X_processed = pd.concat([X_processed, one_hot_encoded], axis = 1)\n",
    "    X_processed  = X_processed.drop('title_sentiment_polarity', axis = 1)\n",
    "\n",
    "    return X_processed, y_processed, dict_means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26715,)\n",
      "(26047,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Indexing a Series with DataFrame is not supported, use the appropriate DataFrame column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pr_X_train, pr_y_train, means \u001b[39m=\u001b[39m preprocess(X_train, y_train)\n",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(X, y, means, train)\u001b[0m\n\u001b[1;32m     37\u001b[0m y \u001b[39m=\u001b[39m y[X[\u001b[39m'\u001b[39m\u001b[39mn_tokens_content\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m X \u001b[39m=\u001b[39m X[X[\u001b[39m'\u001b[39m\u001b[39mn_tokens_content\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[0;32m---> 40\u001b[0m y \u001b[39m=\u001b[39m y[X[np\u001b[39m.\u001b[39;49mlog1p(X[\u001b[39m'\u001b[39;49m\u001b[39mnum_hrefs\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m<\u001b[39;49m \u001b[39m4\u001b[39;49m]]\n\u001b[1;32m     41\u001b[0m y \u001b[39m=\u001b[39m y[X[np\u001b[39m.\u001b[39mlog1p(X[\u001b[39m'\u001b[39m\u001b[39mnum_self_hrefs\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m<\u001b[39m \u001b[39m3\u001b[39m]]\n\u001b[1;32m     42\u001b[0m y \u001b[39m=\u001b[39m y[X[\u001b[39m'\u001b[39m\u001b[39mself_reference_avg_sharess\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m50000\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/pandas/core/series.py:1033\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     key \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(key, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m   1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_values(key)\n\u001b[0;32m-> 1033\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_with(key)\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/pandas/core/series.py:1043\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slice(slobj)\n\u001b[1;32m   1042\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, ABCDataFrame):\n\u001b[0;32m-> 1043\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndexing a Series with DataFrame is not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msupported, use the appropriate DataFrame column\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1047\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1048\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_values_tuple(key)\n",
      "\u001b[0;31mTypeError\u001b[0m: Indexing a Series with DataFrame is not supported, use the appropriate DataFrame column"
     ]
    }
   ],
   "source": [
    "pr_X_train, pr_y_train, means = preprocess(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24351, 41), (26047,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_X_train.shape, pr_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_X_validation, pr_y_validation, _ = preprocess(X_validation, y_validation, means, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # DROP FEATURES\n",
    "# drop_features = ['kw_max_min', 'kw_max_avg', 'kw_min_min', 'url', 'timedelta', 'n_non_stop_words',\n",
    "#                 'n_tokens_content', 'n_non_stop_unique_tokens', 'self_reference_max_shares',\n",
    "#                 'self_reference_min_shares', 'rate_positive_words', 'rate_negative_words',\n",
    "#                 'max_positive_polarity', 'min_positive_polarity', 'min_negative_polarity',\n",
    "#                 'max_negative_polarity', 'abs_title_subjectivity', 'abs_title_sentiment_polarity',\n",
    "#                 'kw_min_max', 'kw_max_max', 'kw_min_avg']\n",
    "# drop_indices = [mapping[value] for value in drop_features]\n",
    "\n",
    "# # -------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # APPLY LOGARITHMS\n",
    "# log1p_features = ['num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'kw_avg_min', 'kw_avg_avg', 'self_reference_avg_sharess']\n",
    "# log1p_indices = [mapping[value] for value in log1p_features]\n",
    "\n",
    "# log_feature = ['shares']\n",
    "# log_index = [mapping[value] for value in log_feature]\n",
    "\n",
    "# # -------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # DISCRETIZE KW_AVG_MAX\n",
    "# conditions = [\n",
    "#               (np.log1p(df['kw_avg_max']) < 2),\n",
    "#               ((np.log1p(df['kw_avg_max']) < 11) & (np.log1p(df['kw_avg_max']) > 0)),\n",
    "#               (np.log1p(df['kw_avg_max']) > 11)\n",
    "# ]\n",
    "\n",
    "# labels = ['kw_avg_max_none', 'kw_avg_max_medium', 'kw_avg_max_high']\n",
    "\n",
    "# df['kw_avg_max'] = np.where(conditions[0], labels[0],\n",
    "#                             np.where(conditions[1], labels[1],\n",
    "#                                      np.where(conditions[2], labels[2], None)))\n",
    "\n",
    "# # ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # DISCRETIZE TITLE_SUBJECTIVITY AND TITLE_SENTIMENT_POLARITY\n",
    "# subjectivity_bin_values = {\n",
    "#                            1 : 'no_subjectivity',\n",
    "#                            2 : 'low_subjectivity',\n",
    "#                            3 : 'medium_subjectvity',\n",
    "#                            4 : 'high_subjectivity'\n",
    "#                           }\n",
    "\n",
    "# subjectivity_bins = [-0.000001, 0.000001, 0.334, 0.667, 1]\n",
    "\n",
    "\n",
    "# df['title_sentiment_polarity'] = pd.cut(df['title_sentiment_polarity'],\n",
    "#                                         bins=[-1.00001, -0.5, -0.000000001, +0.000000001, 0.5, 1],\n",
    "#                                         labels=['high_negative_polarity', 'low_negative_polarity', 'neutral_polarity', 'low_positive_polarity', 'high_positive_polarity'],\n",
    "#                                         right = True)\n",
    "\n",
    "# polarity_bin_values = {\n",
    "#                            1 : 'high_negative_polarity',\n",
    "#                            2 : 'low_negative_polarity',\n",
    "#                            3 : 'neutral_polarity',\n",
    "#                            4 : 'low_positive_polarity',\n",
    "#                            5 : 'high_positive_polarity'\n",
    "#                           }\n",
    "\n",
    "# polarity_bins = [-1.00001, -0.5, -0.000000001, +0.000000001, 0.5, 1]\n",
    "\n",
    "# # Define a function to assign values to bins\n",
    "# def discretize_title_subjectivity(X):\n",
    "#     bin_indices = np.digitize(X[:, 0], subjectivity_bins, right = True)\n",
    "#     bin_labels = np.array([subjectivity_bin_values[i] for i in bin_indices]).reshape(-1, 1)\n",
    "#     return bin_labels\n",
    "\n",
    "# def discretize_title_sentiment_polarity(X):\n",
    "#     bin_indices = np.digitize(X[:, 0], polarity_bins, right = True)\n",
    "#     bin_labels = np.array([polarity_bin_values[i] for i in bin_indices]).reshape(-1, 1)\n",
    "#     return bin_labels\n",
    "\n",
    "# # ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # DISCRETIZE WEEKDAY\n",
    "# df['weekday'] = np.where(df['weekday'].isin(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']), 'Not Weekend', 'Weekend')\n",
    "\n",
    "# # ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # APPLY PIPLINE\n",
    "# preprocessing_pipeline = Pipeline([\n",
    "#     ('preprocessing', ColumnTransformer([                                                                             # Drop useless/correlated features.\n",
    "#         ('log1p', FunctionTransformer(func = np.log1p, inverse_func = np.expm1, validate = False), log1p_indices),  # Apply log1p.\n",
    "#         ('log', FunctionTransformer(func = np.log, inverse_func = np.exp, validate = False), log_index),            # Apply logp.\n",
    "#         ('discretize_subjectivity', FunctionTransformer(func = discretize_title_subjectivity, validate = False), [42]),\n",
    "#         ('discretize_sentiment_polarity', FunctionTransformer(func = discretize_title_sentiment_polarity, validate = False), [43]),\n",
    "#         ('onehot', OneHotEncoder(categories='auto', sparse=False, handle_unknown='ignore'), [18, 42, 43, 46, 47]),\n",
    "#         (\"columnDropper\", columnDropperTransformer, drop_features)],\n",
    "#     remainder = 'passthrough'))\n",
    "#     ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\\-- A BIT OF TUNING AND THEN SELECTION --/#\n",
    "\n",
    "# We first define the grid of parameters. We need to specify both the values AND the name of the classifier because it will be of use\n",
    "# when picking the right ones.\n",
    "# param_grid = {\n",
    "\n",
    "#     'sklearn.ensemble.GradientBoostingRegressor': {                            # Here we specify only the SVM, but there should be a lot of other configurations.\n",
    "#                 'learning_rate': [0.05, 0.1, 0.15],\n",
    "#                 'n_estimator': [100, 300, 500, 1000, 2000],\n",
    "#                 'min_samples_leaf' : [5, 15, 20],\n",
    "#                 'min_weight_fraction_leaf' : [0.0, 0.1, 0.2],\n",
    "#                 'max_depth' : [2, 3, 4, 5],\n",
    "#                 'subsample' : [0.5, 0.75, 1.0],\n",
    "#                 'min_samples_split' : [2, 5, 10, 20],\n",
    "#                 'max_features' : ['auto', 'sqrt', 'log2']           \n",
    "#                }                                     \n",
    "             \n",
    "#              }\n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    'sklearn.ensemble.GradientBoostingRegressor': {                            # Here we specify only the SVM, but there should be a lot of other configurations.\n",
    "                'learning_rate': [0.05, 0.2],\n",
    "                'n_estimator': [500, 1000],\n",
    "                'min_samples_leaf' : [5, 15, 20],\n",
    "                'min_weight_fraction_leaf' : [0.0, 0.1, 0.2],\n",
    "                'max_depth' : [3, 4, 5],\n",
    "                'subsample' : [0.5, 1.0],\n",
    "                'min_samples_split' : [5, 10, 20],\n",
    "                'max_features' : ['auto', 'sqrt', 'log2']           \n",
    "               }                                     \n",
    "             \n",
    "             }\n",
    "\n",
    "scores      = [mean_squared_error] # List of scores we want to compute, e.g. accuracy_score, f1_score, ... They must be methods.\n",
    "THRESH_SKIP = 2\n",
    "THRESH_PERCENTAGE = 0.95\n",
    "MAIN_SCORE = 'mean_squared_error'\n",
    "\n",
    "# Create a StratifiedKFold object for Cross Validation and perform the train/test split.\n",
    "skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = SEED)\n",
    "\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Start the Cross Validation. In this first step we simply try some configurations and try to tune the models the best we can.\n",
    "# Notice that this phase should take not much time, but given the amount of possible configurations it is likely to be quite expensive.\n",
    "# However, we are sure that at the end we will get a fair comparison between every model. \n",
    "# This will be called Cross-VSV (Validation-Selection-Validation) or Cross-SeleDation (Selection-Validation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validator = PrunedCV(X_train, y_train, skf)\n",
    "\n",
    "\n",
    "cross_validator.set_params(param_grid, scores)\n",
    "cross_validator.set_evaluation(accuracy_score, 2, 0.95)\n",
    "toc = time.time()\n",
    "cross_validator.do_cross_validation(verbose = 4)\n",
    "\n",
    "\n",
    "# At the end we will have a fair comparison between models. We can store additional information regarding the Standard Deviation as well.\n",
    "\n",
    "# Consider using PrettyTable() to visualize the results.\n",
    "# t = PrettyTable()\n",
    "# t.field_names = ['Model', 'Accuracy', 'F1']\n",
    "# t.add_row([name, accuracy, f1])\n",
    "\n",
    "# print(t)\n",
    "\n",
    "tic = time.time()\n",
    "print(tic - toc)\n",
    "# CONSIDER TO PRUNE THE SEARCH IF THE RESULTS DON'T IMPROVE MUCH.\n",
    "# Example: if the best result with one model is 0.95 and another configuration of the same model is stuck near 0.90, just don't\n",
    "# wait for the end of the loop but go to the next model/configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [24351, 26047]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m params \u001b[39m=\u001b[39m {                            \u001b[39m# Here we specify only the SVM, but there should be a lot of other configurations.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.05\u001b[39m, \u001b[39m0.2\u001b[39m],\n\u001b[1;32m      3\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mn_estimator\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m500\u001b[39m, \u001b[39m1000\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mmax_features\u001b[39m\u001b[39m'\u001b[39m : [\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msqrt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlog2\u001b[39m\u001b[39m'\u001b[39m]           \n\u001b[1;32m     10\u001b[0m         }\n\u001b[1;32m     12\u001b[0m grad_bost_regr \u001b[39m=\u001b[39m GradientBoostingRegressor()\n\u001b[0;32m---> 13\u001b[0m grad_bost_regr\u001b[39m.\u001b[39;49mfit(pr_X_train, pr_y_train)\n\u001b[1;32m     14\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(grad_bost_regr\u001b[39m.\u001b[39mpredict(X_test))\n\u001b[1;32m     15\u001b[0m rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(mean_squared_error(np\u001b[39m.\u001b[39mexp(y_validation), y_pred))\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:429\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_state()\n\u001b[1;32m    425\u001b[0m \u001b[39m# Check input\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m# Since check_array converts both X and y to the same dtype, but the\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m# trees use different types for X and y, checking them separately.\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    430\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mDTYPE, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    431\u001b[0m )\n\u001b[1;32m    433\u001b[0m sample_weight_is_none \u001b[39m=\u001b[39m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    435\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m-> 1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/Documents/DSL-Online-News-Popularity/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [24351, 26047]"
     ]
    }
   ],
   "source": [
    "params = {                            # Here we specify only the SVM, but there should be a lot of other configurations.\n",
    "          'learning_rate': [0.05, 0.2],\n",
    "          'n_estimator': [500, 1000],\n",
    "          'min_samples_leaf' : [5, 15, 20],\n",
    "          'min_weight_fraction_leaf' : [0.0, 0.1, 0.2],\n",
    "          'max_depth' : [3, 4, 5],\n",
    "          'subsample' : [0.5, 1.0],\n",
    "          'min_samples_split' : [5, 10, 20],\n",
    "          'max_features' : ['auto', 'sqrt', 'log2']           \n",
    "        }\n",
    "\n",
    "grad_bost_regr = GradientBoostingRegressor()\n",
    "grad_bost_regr.fit(pr_X_train, pr_y_train)\n",
    "y_pred = np.exp(grad_bost_regr.predict(X_test))\n",
    "rmse = np.sqrt(mean_squared_error(np.exp(y_validation), y_pred))\n",
    "print(f'GradientBoostingRegressor without PCA')\n",
    "print(f'rmse : {rmse}')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pr_X_train_pca = pca.fit_transform(pr_X_train)\n",
    "# Transform the test data using the trained PCA model\n",
    "pr_X_validation_pca = pca.transform(pr_X_validation)\n",
    "regr_pca = GradientBoostingRegressor()\n",
    "regr_pca.fit(pr_X_train_pca, pr_y_train)\n",
    "y_pred_pca = np.exp(regr_pca.predict(pr_X_validation_pca))\n",
    "mse_pca = mean_squared_error(np.exp(y_validation), y_pred_pca)\n",
    "rmse_pca = np.sqrt(mse_pca)\n",
    "print(f'GradientBoostingRegressor with PCA')\n",
    "print(f'rmse_pca : {rmse_pca}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
